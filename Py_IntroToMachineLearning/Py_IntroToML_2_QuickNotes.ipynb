{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2 notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors\n",
    "\n",
    "K-nearest neighbors \"score\" method\n",
    "\n",
    "K-nearest neighbors decision boundary p. 38\n",
    "\n",
    "May be useful to plot training score vs test score for varying model complexity (larger k = less complex).\n",
    "\n",
    "K-nearest neighbors regression: KNeighborsRegressor, vs KNeighborsClassifier\n",
    "\n",
    "How is R^2 calculated for a knn regression?\n",
    "\n",
    "Similarly for classification: visualize tradeoff between training accuracy (and complexity) and test accuracy, this time by viewing predicted values for a large number of possible test values for different values of k.  \n",
    "\n",
    "\n",
    "\n",
    "Performs poorly with large numbers of features (slow), particularly when the dataset is sparse (many features = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear models!\n",
    "\n",
    "#### Regression\n",
    "\n",
    "OLS: minimizing mean squared error.  With many predictors, overfitting is a danger (R2 for training high, for test low).\n",
    "\n",
    "Ridge: minimizing MSE with a constraint on the coefficients (an example of regularization ie constraint, here L2 regularization), p. 49.  Choosing alpha... (does alpha increase or decrease regularization?)\n",
    "\n",
    "Lasso: permits some coefficients to be exactly 0 (L1 regularization)\n",
    "\n",
    "#### Classification\n",
    "\n",
    "Using logistic regression\n",
    "\n",
    "Using linear support vector machine\n",
    "\n",
    "For both, a parameter C determines regularization, with higher=less regularized (ie more committed to capturing training data)\n",
    "\n",
    "With more than two classes two predict, one approach is to simply make a binary classification model for each vs rest, then apply each in turn to test data and pick the class that gets the highest score. p. 63\n",
    "\n",
    "\n",
    "#### Comments\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes classifiers\n",
    "\n",
    "In general, may be faster but somewhat less accurate than linear models\n",
    "\n",
    "In sklearn, GaussianNB (for continuous), BernoulliNB (binary), MultinomialNB (multinomial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision trees\n",
    "\n",
    "Widely used (for regression as well - how?)\n",
    "\n",
    "The trick is to learn the best sequence of if/else questions; these questions are called \"tests\".  For continuous data, tests are of the form \"is feature a larger than threshold t?\"\n",
    "\n",
    "Two moons dataset: tests amount to performing successive splits of the region.  Depth 1 = single split, Depth 2 = single split plus one split in each resulting region, etc.\n",
    "\n",
    "You can continue partitioning until all leaves are pure (that is, all leaves of the tree contain only one class)\n",
    "\n",
    "pre-pruning involves constraining the depth of the tree.  \n",
    "\n",
    "Feature importance (how is this measured?)\n",
    "\n",
    "Decision trees for regression: note that they are unable to extrapolate beyond the range of the training data.\n",
    "\n",
    "#### Comments\n",
    "\n",
    "Easy to explain and visualize.  Work well for data of varied types.  However, even with pre-pruning they tend to generalize poorly.  \n",
    "\n",
    "### Ensembles of decision trees\n",
    "\n",
    "Random forest: averaging the predictions of many decision trees can reduce overfitting while retaining predictive accuracy.  Randomized in two ways: with a bootstrap sample (a sample of the same size drawn from the original but with replacement) and by choosing a random subset of features as candidates for the best test at each node.  \n",
    "\n",
    "Combining results: for each test datapoint, each tree contributes a probability re: classification; these probabilities are averaged.  \n",
    "\n",
    "Gradient boosting regression trees: not clear just how they work; many shallow trees, each attempting to correct errors of previous.  \n",
    "\n",
    "#### Comments\n",
    "\n",
    "Very flexible and widely-used, though difficult to visualize and explain.  Time-consuming, can be parallelized with n-cores parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernelized Support Vector Machines \n",
    "\n",
    "p. 92"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks\n",
    "\n",
    "p. 104\n",
    "\n",
    "Multilayer perceptrons (MLPs), also known as feed-forward neural networks\n",
    "\n",
    "Think of an MLP as generalizing linear regression/classification: several predictors are weighted to yield an output, and this is repeated through multiple layers.  Input, hidden layer(s), output.  \n",
    "\n",
    "But just computing a weighted sum through one or more hidden layers would yield the same result as 0 hidden layers.  The interesting part is that we introduce nonlinearity, ie, the value at a node is a nonlinear function of the weighted sum.  Rectifying nonlinearity: cuts off values below zero.  Tanh: approaches -1 for low input values and +1 for high input values.  This nonlinearity allows the network to learn more complex relationships than a linear model could. \n",
    "\n",
    "Number of nodes per layer and number of layers are important parameters.  \n",
    "\n",
    "NN can also be regularized with alpha L2 penalty.\n",
    "\n",
    "Useful to visualize decision boundary results for different alphas and numbers of hidden units and different nonlinearity\n",
    "\n",
    "NN's perform best when the data is normalized.\n",
    "\n",
    "#### Comments\n",
    "\n",
    "NN's can outperform other machine learning techniques, but they require careful tuning and can be time-consuming to train.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating uncertainty\n",
    "\n",
    "p. 119 the predict_proba method for most classifiers in sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "\n",
    "It would be nice to return to the Iris dataset and build a decision tree and then a random forest for it\n",
    "\n",
    "**Other interesting projects:**\n",
    "1. Translating power and significance (and severity) into ML - or would these be more appropriate for unsupervised ML? (See https://machinelearningmastery.com/statistical-significance-tests-for-comparing-machine-learning-algorithms/)\n",
    "2. Comparing ridge regression and random forest regression\n",
    "3. Less formally than 1, exploring different packages for producing toy data and demonstrating how ML algorithms can sometimes find structure where there isn't any.  Perhaps start here (lends itself to visualization), but do download that paper.  \n",
    "4. Work with a real dataset perhaps following https://machinelearningmastery.com/self-study-machine-learning-projects/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
